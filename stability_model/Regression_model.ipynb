{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cdb4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import esm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68ea4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "class SeqRegressionDataset(Dataset):\n",
    "    def __init__(self, seqs, ys):\n",
    "        self.seqs = list(seqs)\n",
    "        self.ys = np.asarray(ys, dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seqs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.seqs[idx], self.ys[idx]\n",
    "\n",
    "def make_collate_fn(alphabet):\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "    pad_idx = alphabet.padding_idx\n",
    "\n",
    "    def collate(batch):\n",
    "        seqs, ys = zip(*batch)\n",
    "        data = [(f\"protein{i}\", s) for i, s in enumerate(seqs)]\n",
    "        _, _, tokens = batch_converter(data)  \n",
    "\n",
    "        not_pad = tokens.ne(pad_idx)         \n",
    "        valid_counts = not_pad.sum(dim=1)    \n",
    "        lengths = (valid_counts - 2).clamp(min=0) \n",
    "\n",
    "        B, T = tokens.shape\n",
    "        residue_mask = torch.zeros((B, T), dtype=torch.bool)\n",
    "        for b in range(B):\n",
    "            L = int(lengths[b].item())\n",
    "            if L > 0:\n",
    "                residue_mask[b, 1:1+L] = True\n",
    "\n",
    "        ys_t = torch.tensor(ys, dtype=torch.float32)\n",
    "        return tokens, residue_mask, lengths, ys_t\n",
    "\n",
    "    return collate\n",
    "\n",
    "\n",
    "class GlobalAttnPlusCTermAttnRegressor(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        esm_model,\n",
    "        repr_layer=33,\n",
    "        emb_dim=1280,\n",
    "        cterm_k=6,\n",
    "        attn_hidden=256,\n",
    "        cterm_attn_hidden=128,\n",
    "        head_hidden=(512, 256),\n",
    "        dropout=0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.esm = esm_model\n",
    "        self.repr_layer = repr_layer\n",
    "        self.cterm_k = cterm_k\n",
    "\n",
    "        self.global_attn = nn.Sequential(\n",
    "            nn.Linear(emb_dim, attn_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(attn_hidden, 1),\n",
    "        )\n",
    "\n",
    "        self.cterm_attn = nn.Sequential(\n",
    "            nn.Linear(emb_dim, cterm_attn_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(cterm_attn_hidden, 1),\n",
    "        )\n",
    "\n",
    "        layers = []\n",
    "        in_dim = emb_dim * 2\n",
    "        for h in head_hidden:\n",
    "            layers += [nn.Linear(in_dim, h), nn.ReLU(), nn.Dropout(dropout)]\n",
    "            in_dim = h\n",
    "        layers += [nn.Linear(in_dim, 1)]\n",
    "        self.head = nn.Sequential(*layers)\n",
    "\n",
    "        for p in self.esm.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.esm.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _esm_forward(self, tokens):\n",
    "        out = self.esm(tokens, repr_layers=[self.repr_layer], return_contacts=False)\n",
    "        return out[\"representations\"][self.repr_layer] \n",
    "\n",
    "    def forward(self, tokens, residue_mask, lengths):\n",
    "        H = self._esm_forward(tokens)  \n",
    "\n",
    "        g_logits = self.global_attn(H).squeeze(-1)                 \n",
    "        g_logits = g_logits.masked_fill(~residue_mask, -1e9)\n",
    "        g_w = F.softmax(g_logits, dim=1)                          \n",
    "        global_vec = torch.sum(H * g_w.unsqueeze(-1), dim=1)      \n",
    "\n",
    "        B, T, D = H.shape\n",
    "        cterm_mask = torch.zeros((B, T), dtype=torch.bool, device=H.device)\n",
    "        for b in range(B):\n",
    "            L = int(lengths[b].item())\n",
    "            if L <= 0:\n",
    "                continue\n",
    "            k = min(self.cterm_k, L)\n",
    "            start = 1 + (L - k)    \n",
    "            end = 1 + L\n",
    "            cterm_mask[b, start:end] = True\n",
    "\n",
    "        c_logits = self.cterm_attn(H).squeeze(-1)                  \n",
    "        c_logits = c_logits.masked_fill(~cterm_mask, -1e9)\n",
    "        c_w = F.softmax(c_logits, dim=1)\n",
    "        cterm_vec = torch.sum(H * c_w.unsqueeze(-1), dim=1)       \n",
    "\n",
    "        feat = torch.cat([global_vec, cterm_vec], dim=1)          \n",
    "        y_hat = self.head(feat).squeeze(-1)                        \n",
    "        return y_hat\n",
    "\n",
    "\n",
    "def collect_preds(model, loader, device, clip_01=True):\n",
    "    model.eval()\n",
    "    ys_true, ys_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for tokens, residue_mask, lengths, ys in loader:\n",
    "            tokens = tokens.to(device)\n",
    "            residue_mask = residue_mask.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "\n",
    "            pred = model(tokens, residue_mask, lengths)  \n",
    "            pred = pred.detach().cpu().numpy()\n",
    "\n",
    "            if clip_01:\n",
    "                pred = np.clip(pred, 0.0, 1.0)\n",
    "\n",
    "            ys_true.append(ys.numpy())\n",
    "            ys_pred.append(pred)\n",
    "\n",
    "    y_true = np.concatenate(ys_true)\n",
    "    y_pred = np.concatenate(ys_pred)\n",
    "    return y_true, y_pred\n",
    "\n",
    "\n",
    "def eval_metrics(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rho, p = spearmanr(y_true, y_pred)\n",
    "    return mae, rmse, r2, rho, p\n",
    "\n",
    "\n",
    "def pairwise_ranking_loss(pred, y, margin=0.05, n_pairs=2048, min_label_diff=0.02):\n",
    "    B = y.size(0)\n",
    "    if B < 2:\n",
    "        return pred.new_tensor(0.0)\n",
    "\n",
    "    i = torch.randint(0, B, (n_pairs,), device=pred.device)\n",
    "    j = torch.randint(0, B, (n_pairs,), device=pred.device)\n",
    "\n",
    "    yi, yj = y[i], y[j]\n",
    "    pi, pj = pred[i], pred[j]\n",
    "\n",
    "    diff = yi - yj\n",
    "    keep = diff.abs() > min_label_diff\n",
    "    if keep.sum() < 1:\n",
    "        return pred.new_tensor(0.0)\n",
    "\n",
    "    diff = diff[keep]\n",
    "    pi, pj = pi[keep], pj[keep]\n",
    "    s = torch.sign(diff)  # +1 if yi>yj else -1\n",
    "\n",
    "    # hinge ranking loss\n",
    "    loss = torch.relu(margin - s * (pi - pj)).mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_one_fold(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    device,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    max_epochs=40,\n",
    "    patience=6,\n",
    "    use_ranking_loss=True,\n",
    "    rank_lambda=0.8,       \n",
    "    rank_margin=0.05,\n",
    "    rank_pairs=2048,\n",
    "    min_label_diff=0.02,\n",
    "):\n",
    "    model.to(device)\n",
    "\n",
    "    mse_loss = nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_rho = -1e9\n",
    "    best_state = None\n",
    "    no_improve = 0\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        n = 0\n",
    "\n",
    "        for tokens, residue_mask, lengths, ys in train_loader:\n",
    "            tokens = tokens.to(device)\n",
    "            residue_mask = residue_mask.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "            ys = ys.to(device)\n",
    "\n",
    "            pred_raw = model(tokens, residue_mask, lengths)  \n",
    "            if use_ranking_loss:\n",
    "                loss_rank = pairwise_ranking_loss(\n",
    "                    pred_raw, ys,\n",
    "                    margin=rank_margin,\n",
    "                    n_pairs=rank_pairs,\n",
    "                    min_label_diff=min_label_diff\n",
    "                )\n",
    "                loss_mse = mse_loss(pred_raw, ys)\n",
    "                loss = rank_lambda * loss_rank + (1.0 - rank_lambda) * loss_mse\n",
    "            else:\n",
    "                loss = mse_loss(pred_raw, ys)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * ys.size(0)\n",
    "            n += ys.size(0)\n",
    "\n",
    "        train_loss = total_loss / max(n, 1)\n",
    "\n",
    "        y_val_true, y_val_pred = collect_preds(model, val_loader, device, clip_01=True)\n",
    "        val_mae, val_rmse, val_r2, val_rho, val_p = eval_metrics(y_val_true, y_val_pred)\n",
    "\n",
    "        print(\n",
    "            f\"  Epoch {epoch:02d} | train_loss={train_loss:.4f} | \"\n",
    "            f\"val_Spearmanρ={val_rho:.4f} (p={val_p:.2e}) | \"\n",
    "            f\"val_R2={val_r2:.4f} val_MAE={val_mae:.4f} val_RMSE={val_rmse:.4f}\"\n",
    "        )\n",
    "\n",
    "        if val_rho > best_val_rho + 1e-4:\n",
    "            best_val_rho = val_rho\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    y_val_true, y_val_pred = collect_preds(model, val_loader, device, clip_01=True)\n",
    "    val_mae, val_rmse, val_r2, val_rho, val_p = eval_metrics(y_val_true, y_val_pred)\n",
    "    return model, (val_mae, val_rmse, val_r2, val_rho, val_p)\n",
    "\n",
    "\n",
    "def run_5fold_cv(\n",
    "    sequences,\n",
    "    y,\n",
    "    device,\n",
    "    n_splits=5,\n",
    "    seed=42,\n",
    "    batch_size=8,\n",
    "    cterm_k=6,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    max_epochs=40,\n",
    "    patience=6,\n",
    "    save_dir=None,\n",
    "    y_bin_q=10,            \n",
    "    use_ranking_loss=True,\n",
    "    rank_lambda=0.8,\n",
    "    rank_margin=0.05,\n",
    "    rank_pairs=2048,\n",
    "    min_label_diff=0.02,\n",
    "):\n",
    "    set_seed(seed)\n",
    "\n",
    "    y_bins = pd.qcut(y, q=y_bin_q, labels=False, duplicates=\"drop\")\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "\n",
    "    fold_results = []\n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(np.zeros(len(y)), y_bins), start=1):\n",
    "        print(f\"\\n==== Fold {fold}/{n_splits} ====\")\n",
    "\n",
    "        X_tr = [sequences[i] for i in tr_idx]\n",
    "        y_tr = y[tr_idx]\n",
    "        X_va = [sequences[i] for i in va_idx]\n",
    "        y_va = y[va_idx]\n",
    "\n",
    "        esm_model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "        collate_fn = make_collate_fn(alphabet)\n",
    "\n",
    "        train_ds = SeqRegressionDataset(X_tr, y_tr)\n",
    "        val_ds = SeqRegressionDataset(X_va, y_va)\n",
    "\n",
    "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "        val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "        model = GlobalAttnPlusCTermAttnRegressor(\n",
    "            esm_model=esm_model,\n",
    "            repr_layer=33,\n",
    "            emb_dim=1280,\n",
    "            cterm_k=cterm_k,\n",
    "            attn_hidden=256,\n",
    "            cterm_attn_hidden=128,\n",
    "            head_hidden=(512, 256),\n",
    "            dropout=0.2,\n",
    "        )\n",
    "\n",
    "        model, metrics = train_one_fold(\n",
    "            model,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            device=device,\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay,\n",
    "            max_epochs=max_epochs,\n",
    "            patience=patience,\n",
    "            use_ranking_loss=use_ranking_loss,\n",
    "            rank_lambda=rank_lambda,\n",
    "            rank_margin=rank_margin,\n",
    "            rank_pairs=rank_pairs,\n",
    "            min_label_diff=min_label_diff,\n",
    "        )\n",
    "\n",
    "        val_mae, val_rmse, val_r2, val_rho, val_p = metrics\n",
    "        fold_results.append(metrics)\n",
    "        print(\n",
    "            f\"Fold {fold} best | Spearmanρ={val_rho:.4f} (p={val_p:.2e}) | \"\n",
    "            f\"val_R2={val_r2:.4f} val_MAE={val_mae:.4f} val_RMSE={val_rmse:.4f}\"\n",
    "        )\n",
    "\n",
    "        if save_dir is not None:\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            out_path = os.path.join(save_dir, f\"fold{fold}_best_head.pth\")\n",
    "            torch.save(model.state_dict(), out_path)\n",
    "            print(\"  Saved:\", out_path)\n",
    "\n",
    "    fold_results = np.array(fold_results, dtype=float) \n",
    "\n",
    "    mae_mean, rmse_mean, r2_mean, rho_mean = (\n",
    "        fold_results[:, 0].mean(),\n",
    "        fold_results[:, 1].mean(),\n",
    "        fold_results[:, 2].mean(),\n",
    "        fold_results[:, 3].mean(),\n",
    "    )\n",
    "    mae_std, rmse_std, r2_std, rho_std = (\n",
    "        fold_results[:, 0].std(ddof=1),\n",
    "        fold_results[:, 1].std(ddof=1),\n",
    "        fold_results[:, 2].std(ddof=1),\n",
    "        fold_results[:, 3].std(ddof=1),\n",
    "    )\n",
    "\n",
    "    print(\"\\n==== CV Summary (mean ± std) ====\")\n",
    "    print(f\"Spearmanρ  = {rho_mean:.4f} ± {rho_std:.4f}\")\n",
    "    print(f\"val_R2     = {r2_mean:.4f} ± {r2_std:.4f}\")\n",
    "    print(f\"val_MAE    = {mae_mean:.4f} ± {mae_std:.4f}\")\n",
    "    print(f\"val_RMSE   = {rmse_mean:.4f} ± {rmse_std:.4f}\")\n",
    "\n",
    "    return fold_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869fa35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    set_seed(42)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "    train_data = pd.read_csv(\"./Regression_train_data.csv\")\n",
    "    df = train_data.copy()\n",
    "    sequences = df[\"protein_sequence\"].astype(str).tolist()\n",
    "    y = np.clip(df[\"mean_stability\"].astype(float).to_numpy(), 0.0, 1.0)\n",
    "\n",
    "    results = run_5fold_cv(\n",
    "        sequences=sequences,\n",
    "        y=y,\n",
    "        device=device,\n",
    "        n_splits=5,\n",
    "        seed=42,\n",
    "        batch_size=16,          \n",
    "        cterm_k=6,\n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-4,\n",
    "        max_epochs=40,\n",
    "        patience=6,\n",
    "        save_dir=\"./cv_models\",\n",
    "        y_bin_q=6,              \n",
    "        use_ranking_loss=True,\n",
    "        rank_lambda=0.75,      \n",
    "        rank_margin=0.05,\n",
    "        rank_pairs=4096,     \n",
    "        min_label_diff=0.02,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
